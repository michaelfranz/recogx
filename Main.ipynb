{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# We define the data transformations and loaders to load the images and their corresponding\n",
    "# labels from the training folder. We use the Adam optimizer and cross-entropy loss function for training.\n",
    "# Finally, we train the model for 10 epochs and save the model weights.\n",
    "# The training data is loaded using PyTorch's DataLoader, and the model is trained using the Adam optimizer and\n",
    "# cross-entropy loss. The trained model is then saved to a file for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import GenderClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define the data transformations and loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((12, 98)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = datasets.ImageFolder('mfc_dataset_train_test/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "classes = train_loader.dataset.classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 12, 98])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 = female\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label} = {classes[label]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3580154614376688\n",
      "Epoch 2, Loss: 1.0831500261355513\n",
      "Epoch 3, Loss: 0.6995161408052658\n",
      "Epoch 4, Loss: 0.6992301919970649\n",
      "Epoch 5, Loss: 0.6993988961838304\n",
      "Epoch 6, Loss: 0.6996186049982381\n",
      "Epoch 7, Loss: 0.6998278005435444\n",
      "Epoch 8, Loss: 0.7000098093249165\n",
      "Epoch 9, Loss: 0.7001556527500336\n",
      "Epoch 10, Loss: 0.7002681328084903\n",
      "Epoch 11, Loss: 0.7003534712349645\n",
      "Epoch 12, Loss: 0.7004175161401304\n",
      "Epoch 13, Loss: 0.7004652769801716\n",
      "Epoch 14, Loss: 0.7005007156548789\n",
      "Epoch 15, Loss: 0.7005268988517908\n",
      "Epoch 16, Loss: 0.700546230371006\n",
      "Epoch 17, Loss: 0.7005604747385262\n",
      "Epoch 18, Loss: 0.7005709261178209\n",
      "Epoch 19, Loss: 0.7005786425389421\n",
      "Epoch 20, Loss: 0.7005842916500835\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GenderClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'saved_model/weights')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.0 %\n"
     ]
    }
   ],
   "source": [
    "test_set = datasets.ImageFolder('mfc_dataset_train_test/test', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: {} %'.format(100 * correct / total))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
